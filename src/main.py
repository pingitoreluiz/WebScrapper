from playwright.sync_api import sync_playwright
from src.scrapers.kabum import KabumScraper
from src.scrapers.terabyte import TerabyteScraper
from src.scrapers.pichau import PichauScraper
import src.database as database
import argparse
import sys
from datetime import datetime
from src.legacy_utils import setup_logger, print_header, print_stats, format_duration


def parse_arguments():
    """Parse argumentos da linha de comando"""
    parser = argparse.ArgumentParser(
        description="Scraper de preÃ§os de placas de vÃ­deo de mÃºltiplas lojas",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python main.py                          # Executa todos os scrapers
  python main.py --scraper pichau         # Executa apenas Pichau
  python main.py --scraper kabum terabyte # Executa Kabum e Terabyte
  python main.py --headless               # Executa em modo headless
  python main.py --export csv             # Exporta resultados para CSV apÃ³s scraping
        """,
    )

    parser.add_argument(
        "--scraper",
        "-s",
        nargs="+",
        choices=["pichau", "kabum", "terabyte", "all"],
        default=["all"],
        help="Scrapers para executar (padrÃ£o: all)",
    )

    parser.add_argument(
        "--headless",
        action="store_true",
        help="Executar em modo headless (sem interface grÃ¡fica)",
    )

    parser.add_argument(
        "--export", choices=["csv", "json", "both"], help="Exportar dados apÃ³s scraping"
    )

    parser.add_argument(
        "--stats", action="store_true", help="Mostrar estatÃ­sticas do banco de dados"
    )

    return parser.parse_args()


def get_scrapers(scraper_names, headless=False):
    """
    Retorna lista de scrapers baseado nos nomes

    Args:
        scraper_names: Lista de nomes de scrapers
        headless: Se deve executar em modo headless

    Returns:
        Lista de instÃ¢ncias de scrapers
    """
    scraper_map = {
        "pichau": PichauScraper,
        "kabum": KabumScraper,
        "terabyte": TerabyteScraper,
    }

    # Se 'all' estÃ¡ na lista, retorna todos
    if "all" in scraper_names:
        scraper_names = ["pichau", "kabum", "terabyte"]

    scrapers = []
    for name in scraper_names:
        if name in scraper_map:
            scrapers.append(scraper_map[name](headless=headless))

    return scrapers


def main():
    """FunÃ§Ã£o principal"""
    args = parse_arguments()

    # Setup logger
    logger = setup_logger("main", level="INFO")

    # CabeÃ§alho
    print_header("ğŸ›’ SCRAPER DE PREÃ‡OS - PLACAS DE VÃDEO")
    print(f"â° InÃ­cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ Scrapers: {', '.join(args.scraper)}")
    print(f"ğŸ‘ï¸ Modo: {'Headless' if args.headless else 'Visual'}")
    print()

    # Garante que o banco existe
    logger.info("Inicializando banco de dados...")
    database.create_table()

    # Mostra estatÃ­sticas iniciais se solicitado
    if args.stats:
        print("\nğŸ“Š ESTATÃSTICAS INICIAIS")
        print("=" * 60)
        stats = database.get_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        print()

    # ObtÃ©m scrapers
    scrapers = get_scrapers(args.scraper, headless=args.headless)

    if not scrapers:
        logger.error("Nenhum scraper vÃ¡lido selecionado!")
        sys.exit(1)

    # Executa scrapers
    start_time = datetime.now()
    successful = 0
    failed = 0

    # Executa scrapers
    start_time = datetime.now()
    successful = 0
    failed = 0

    # Run async
    import asyncio

    async def run_async_scrapers():
        nonlocal successful, failed
        for scraper in scrapers:
            scraper_name = scraper.__class__.__name__.replace("Scraper", "")
            logger.info(f"Iniciando {scraper_name}...")

            try:
                # The run method is now async and handles its own browser resource management
                # It does not take arguments
                await scraper.run()
                successful += 1
                logger.info(f"âœ… {scraper_name} concluÃ­do com sucesso")
            except KeyboardInterrupt:
                logger.warning(f"âš ï¸ {scraper_name} interrompido pelo usuÃ¡rio")
                print("\n\nğŸ›‘ ExecuÃ§Ã£o interrompida pelo usuÃ¡rio")
                break
            except Exception as e:
                failed += 1
                logger.error(f"âŒ Erro fatal em {scraper_name}: {e}")
                print(f"\nâŒ Erro em {scraper_name}: {e}\n")

    try:
        asyncio.run(run_async_scrapers())
    except KeyboardInterrupt:
        pass

    # Calcula duraÃ§Ã£o
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    # EstatÃ­sticas finais
    print("\n" + "=" * 60)
    print("ğŸ“Š RELATÃ“RIO FINAL")
    print("=" * 60)
    print(f"â±ï¸ DuraÃ§Ã£o total: {format_duration(duration)}")
    print(f"âœ… Scrapers bem-sucedidos: {successful}")
    print(f"âŒ Scrapers com falha: {failed}")
    print()

    # EstatÃ­sticas do banco
    stats = database.get_stats()
    print("ğŸ“ˆ EstatÃ­sticas do Banco:")
    for key, value in stats.items():
        print(f"  â€¢ {key}: {value}")
    print()

    # ExportaÃ§Ã£o se solicitada
    if args.export:
        print("ğŸ“¤ Exportando dados...")

        if args.export in ["csv", "both"]:
            csv_file = database.export_to_csv()
            print(f"  âœ“ CSV: {csv_file}")

        if args.export in ["json", "both"]:
            json_file = database.export_to_json()
            print(f"  âœ“ JSON: {json_file}")

        print()

    # Mensagem final
    print_header("âœ… COLETA FINALIZADA")
    logger.info("Programa encerrado")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Programa interrompido pelo usuÃ¡rio")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Erro fatal: {e}")
        sys.exit(1)
